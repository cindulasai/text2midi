# Feature Specification: MidiGent V2 - Agentic Music Generation with True Randomization

**Feature Branch**: `v2-agentic-generation`  
**Created**: January 31, 2026  
**Status**: Draft  
**Input**: User description: "Complete rewrite to fix identical output bug and implement true agentic music generation with proper randomization, variation, and all V1 features"

## Executive Summary

MidiGent V2 is a complete architectural overhaul that addresses critical flaws in V1 while preserving and enhancing all existing functionality:

**CRITICAL BUG FIXED**: V1 generates identical tracks on every run because `random` module is never seeded with entropy. Every generation uses the same pseudo-random sequence.

**V2 Core Principles**:
1. **True Randomization**: Seed RNG with time + session ID for unique outputs every time
2. **Agentic Generation**: Each track generation is an independent agent with its own creative decisions
3. **Variation Engine**: Built-in variation system ensures no two generations are alike
4. **All V1 Features Preserved**: Duration parsing, multi-track, cultural awareness, humanization
5. **Session-Based Memory**: Conversational context with build/modify capabilities

## User Scenarios & Testing *(mandatory)*

### User Story 1 - No More Identical Outputs (Priority: P1)

**User Journey**: As a music creator, when I generate music with the same prompt twice, I should get different variations each time - not identical copies.

**Why this priority**: This is a SHOWSTOPPER bug. Current system is fundamentally broken and produces zero variation.

**Independent Test**: Run "generate pop music" 5 times â†’ Get 5 different melodic patterns, rhythms, and variations. No two should be identical.

**Acceptance Scenarios**:

1. **Given** prompt "generate pop music", **When** run twice with app restart, **Then** MIDI files have different note patterns
2. **Given** prompt "5 track jazz", **When** generated twice, **Then** track arrangements differ (different instruments chosen)
3. **Given** prompt "ambient music", **When** generated 10 times, **Then** get 10 unique variations
4. **Given** any prompt, **When** comparing outputs, **Then** note timings, velocities, and patterns measurably different (>30% difference)
5. **Given** same genre/style request, **When** regenerated, **Then** tempo, key, and instrument choices vary naturally
6. **Given** user restarts application, **When** using same prompt, **Then** still generates different music (not deterministic)

---

### User Story 2 - Agentic Track Generation (Priority: P1)

**User Journey**: As a musician, I want each track to be generated by an intelligent agent that makes creative decisions based on musical context, not rigid templates.

**Why this priority**: Enables professional, creative output that sounds human-composed, not algorithmic.

**Independent Test**: Generate 6-track composition â†’ Each track demonstrates context-aware decisions (bass follows harmony, drums complement rhythm, melody uses scale intelligently).

**Acceptance Scenarios**:

1. **Given** multi-track generation, **When** bass agent generates, **Then** follows chord progression intelligently
2. **Given** melody agent, **When** generating, **Then** uses motif repetition and variation (not random walks)
3. **Given** drum agent, **When** in verse vs chorus, **Then** adapts complexity and patterns contextually
4. **Given** harmony agent, **When** generating, **Then** voice leading follows musical rules
5. **Given** counter-melody agent, **When** generating, **Then** complements main melody without clashing
6. **Given** all agents, **When** working together, **Then** produce cohesive composition (not random layers)

---

### User Story 3 - Correct Multi-Track Count (Priority: P1)

**User Journey**: As a music producer, when I request "5 tracks" or "8 tracks," the system generates exactly that number.

**Why this priority**: V1 has critical bug where requested track count is ignored.

**Independent Test**: Request "generate 7 track orchestral music" â†’ MIDI file contains exactly 7 tracks.

**Acceptance Scenarios**:

1. **Given** "5 track pop music", **When** generated, **Then** MIDI has exactly 5 tracks
2. **Given** "8 track cinematic", **When** generated, **Then** MIDI has exactly 8 tracks  
3. **Given** "minimal 2 track", **When** generated, **Then** MIDI has exactly 2 tracks
4. **Given** AI suggests 6 tracks, **When** user doesn't specify, **Then** generates all 6 suggested tracks
5. **Given** explicit count conflicts with AI suggestion, **When** generating, **Then** user's count takes precedence
6. **Given** track generation, **When** complete, **Then** confirmation shows actual track count generated

---

### User Story 4 - Cultural Music Generation (Priority: P1)

**User Journey**: As a global music creator, I request "Japanese traditional music" or "Carnatic music" and get culturally authentic instruments, scales, and rhythms.

**Why this priority**: Essential for world-class, inclusive music generator. V1 feature that must be preserved.

**Independent Test**: Request "Sufi devotional music" â†’ Uses Middle Eastern scales (maqam), oud/ney instruments, spiritual rhythm patterns.

**Acceptance Scenarios**:

1. **Given** "Japanese traditional", **When** generated, **Then** uses koto/shamisen/shakuhachi + pentatonic scales
2. **Given** "Carnatic music", **When** generated, **Then** uses veena/mridangam + Carnatic ragas
3. **Given** "Sufi music", **When** generated, **Then** uses oud/ney/qanun + maqam scales
4. **Given** "Irish folk", **When** generated, **Then** uses fiddle/tin whistle + Celtic modes
5. **Given** "Brazilian samba", **When** generated, **Then** uses cavaquinho/pandeiro + samba rhythms
6. **Given** cultural request, **When** generated, **Then** AI explains instrument/scale choices in output

---

### User Story 5 - Dynamic Section Variation (Priority: P2)

**User Journey**: As a producer, I want generated music to have distinct sections (intro/verse/chorus/bridge/outro) where ALL tracks evolve and change.

**Why this priority**: Professional music has structure and dynamic contrast. V1 feature enhanced in V2.

**Independent Test**: Generate 2-minute track â†’ Verify intro â‰  verse â‰  chorus in energy, density, and patterns across ALL tracks.

**Acceptance Scenarios**:

1. **Given** 2-minute composition, **When** analyzing, **Then** has intro(8bars)/verse(16)/chorus(16)/bridge(8)/outro(8)
2. **Given** chorus section, **When** playing, **Then** ALL tracks increase energy/density vs verse
3. **Given** bridge section, **When** playing, **Then** ALL tracks provide contrast (different harmonies/patterns)
4. **Given** intro section, **When** playing, **Then** gradual build (tracks enter progressively)
5. **Given** outro section, **When** playing, **Then** gradual fade (tracks exit progressively)
6. **Given** section transitions, **When** crossing boundaries, **Then** changes audible across all tracks

---

### User Story 6 - Humanization & Natural Feel (Priority: P2)

**User Journey**: As a musician, I want generated music to sound human-performed with micro-timing variations, velocity dynamics, and groove - not robotic.

**Why this priority**: Distinguishes professional output from obviously computer-generated music. V1 feature preserved.

**Independent Test**: Generate any track â†’ MIDI analysis shows Â±5-20ms timing variations and Â±5-15 velocity variations.

**Acceptance Scenarios**:

1. **Given** any melody, **When** examining timings, **Then** notes have Â±5-20ms micro-variations
2. **Given** any melody, **When** examining velocities, **Then** notes have Â±5-15 velocity variations
3. **Given** drum track, **When** examining hi-hats, **Then** natural velocity variations (not uniform)
4. **Given** rhythmic elements, **When** playing, **Then** subtle swing/groove present (not rigid grid)
5. **Given** downbeats, **When** examining, **Then** emphasized with higher velocities
6. **Given** phrases, **When** examining, **Then** has crescendos/decrescendos (musical expression)

---

### User Story 7 - Flexible Duration Handling (Priority: P2)

**User Journey**: As a content creator, I specify "30 seconds" or "5 minutes" and get exactly that duration with appropriate structure.

**Why this priority**: Real-world use requires precise duration control. V1 feature preserved.

**Independent Test**: Request "generate 4 minute ambient track" â†’ MIDI is 240 seconds (Â±5s) with extended structure.

**Acceptance Scenarios**:

1. **Given** "30 seconds", **When** generated, **Then** creates concise intro-main-outro (30s Â±2s)
2. **Given** "2 minutes", **When** generated, **Then** full structure intro-verse-chorus-bridge-outro (120s Â±5s)
3. **Given** "5 minutes", **When** generated, **Then** extended with verse/chorus repetitions (300s Â±10s)
4. **Given** "45 seconds", **When** generated, **Then** adapts structure to fit timeframe
5. **Given** "8 minutes", **When** generated, **Then** creates long-form composition with development
6. **Given** no duration specified, **When** generated, **Then** defaults to 2 minutes

---

### User Story 8 - Occasion-Based Generation (Priority: P3)

**User Journey**: As a non-musician, I request "party music" or "meditation music" and get contextually appropriate music without knowing music theory.

**Why this priority**: Makes system accessible to everyone. V1 feature preserved.

**Independent Test**: Request "workout music" â†’ High energy (130-150 BPM), driving rhythm, motivational feel.

**Acceptance Scenarios**:

1. **Given** "party music", **When** generated, **Then** high tempo (120-130 BPM), energetic, dance-oriented
2. **Given** "meditation music", **When** generated, **Then** slow (60-70 BPM), ambient, calming
3. **Given** "workout music", **When** generated, **Then** high energy (130-150 BPM), motivational
4. **Given** "cinema background", **When** generated, **Then** cinematic/orchestral, emotional depth
5. **Given** "restaurant background", **When** generated, **Then** moderate tempo, jazz/lounge, unobtrusive
6. **Given** "dramatic scene", **When** generated, **Then** tension-building, dynamic swells

---

### User Story 9 - Session-Based Conversation (Priority: P3)

**User Journey**: As a composer, I can build on previous generations ("now add strings", "make it funkier") within a session.

**Why this priority**: Enables iterative, conversational music creation. V2 enhancement.

**Independent Test**: Generate track â†’ Request "add jazz piano" â†’ New MIDI includes original tracks + piano.

**Acceptance Scenarios**:

1. **Given** existing composition, **When** "add strings", **Then** new track added to existing MIDI
2. **Given** existing composition, **When** "make drums more intense", **Then** drum track modified
3. **Given** existing composition, **When** "extend by 30 seconds", **Then** composition extended
4. **Given** existing composition, **When** "change to minor key", **Then** all tracks transposed
5. **Given** multiple modifications, **When** requested, **Then** session maintains all context
6. **Given** "start over", **When** requested, **Then** session resets cleanly

---

### Edge Cases

- What happens when user requests 15 tracks (exceeds 8-track limit)?
  â†’ System caps at 8, explains: "Generated 8 tracks (system maximum). Consider combining tracks."

- What happens when cultural style + genre conflict ("Japanese techno")?
  â†’ System intelligently blends: Japanese instruments/scales + techno rhythm/energy

- What happens when duration exceeds 10 minutes?
  â†’ System caps at 10 minutes, notifies user of limit

- What happens when invalid duration format ("fiddy minutes")?
  â†’ System uses default 2 minutes, provides feedback on valid formats

- What happens when RNG seed collision (extremely rare)?
  â†’ Timestamp microseconds + UUID ensures uniqueness (probability < 1 in 10^15)

- What happens when AI track planner fails?
  â†’ Fallback to genre-based defaults (popâ†’4 tracks, orchestralâ†’6 tracks, etc.)

## Requirements *(mandatory)*

### Functional Requirements

#### Core Generation
- **FR-001**: System MUST seed random number generator with `time.time() + hash(session_id)` before EVERY generation
- **FR-002**: System MUST generate unique output for identical prompts across application restarts
- **FR-003**: System MUST generate exactly the number of tracks requested by user (1-8 tracks)
- **FR-004**: System MUST validate track count and cap at 8 with user notification
- **FR-005**: System MUST preserve all notes for all tracks in final MIDI file (no track loss)

#### Variation & Randomization
- **FR-006**: System MUST introduce â‰¥30% variation in note patterns between identical prompt generations
- **FR-007**: System MUST vary tempo (Â±10 BPM), key (among compatible keys), and instrumentation on regeneration
- **FR-008**: System MUST apply humanization (timing Â±5-20ms, velocity Â±5-15) to all notes
- **FR-009**: System MUST use variation engine to ensure no deterministic patterns
- **FR-010**: System MUST log seed values for reproducibility when requested (debug mode)

#### Cultural Awareness
- **FR-011**: System MUST recognize â‰¥20 cultural music styles (Japanese, Carnatic, Sufi, Irish, Brazilian, etc.)
- **FR-012**: System MUST select culturally appropriate instruments from knowledge base
- **FR-013**: System MUST use culturally appropriate scales/modes from knowledge base
- **FR-014**: System MUST apply culturally appropriate rhythm patterns from knowledge base
- **FR-015**: System MUST explain cultural choices in generation output

#### Duration Handling
- **FR-016**: System MUST parse natural language duration ("5 minutes", "30 seconds", "2 min")
- **FR-017**: System MUST parse musical duration ("32 bars", "16 bars")
- **FR-018**: System MUST parse time format ("2:30", "1:45")
- **FR-019**: System MUST generate music within Â±5% of requested duration
- **FR-020**: System MUST adapt section structure to requested duration

#### Section Structure
- **FR-021**: System MUST generate distinct sections for compositions (intro/verse/chorus/bridge/outro)
- **FR-022**: System MUST vary energy levels across sections (intro<verse<chorus, bridge=contrast)
- **FR-023**: System MUST vary density (active notes/tracks) across sections
- **FR-024**: System MUST apply section-specific patterns to ALL active tracks
- **FR-025**: System MUST implement smooth transitions between sections

#### Agentic Generation
- **FR-026**: System MUST implement independent generation agents per track type (melody, harmony, bass, drums, etc.)
- **FR-027**: Each agent MUST make context-aware decisions based on musical rules
- **FR-028**: Bass agent MUST follow chord progressions
- **FR-029**: Melody agent MUST use motif development (not random walks)
- **FR-030**: Drum agent MUST adapt patterns to sections
- **FR-031**: Agents MUST coordinate to produce cohesive composition

#### Session Management
- **FR-032**: System MUST maintain session state across user interactions
- **FR-033**: System MUST support "add track" action to existing composition
- **FR-034**: System MUST support "modify track" action to existing composition
- **FR-035**: System MUST support "extend duration" action to existing composition
- **FR-036**: System MUST support "start over" action to reset session
- **FR-037**: System MUST preserve generation history for undo/reference

#### User Experience
- **FR-038**: System MUST provide real-time generation status updates
- **FR-039**: System MUST confirm track count after generation
- **FR-040**: System MUST display generation parameters (tempo, key, duration, tracks)
- **FR-041**: System MUST show cultural/occasion interpretation
- **FR-042**: System MUST provide helpful error messages for invalid requests

### Key Entities

- **GenerationSession**: Represents user session with unique ID, creation time, state, history
- **Track**: Represents single MIDI track with type, instrument, notes, channel, program
- **Note**: Represents single MIDI note with pitch, start_time, duration, velocity, channel
- **Section**: Represents song section with name, bar_range, energy_level, density_level, characteristics
- **TrackAgent**: Intelligent generator for specific track type with decision-making logic
- **VariationEngine**: Manages randomization and ensures uniqueness across generations
- **CulturalKnowledgeBase**: Database of cultural music styles, instruments, scales, rhythms
- **DurationRequest**: Parsed duration with value, unit, validation, conversion methods

## Technical Design

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MidiGent V2 Architecture                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Prompt
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VARIATION ENGINE (NEW)                          â”‚
â”‚  - Seed RNG: time.time() + hash(session_id)     â”‚
â”‚  - Track variation state                         â”‚
â”‚  - Ensure uniqueness                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENHANCED INTENT PARSER                          â”‚
â”‚  - Cultural Music Interpreter                    â”‚
â”‚  - Duration Parser (V1 preserved)                â”‚
â”‚  - Track Count Extractor                         â”‚
â”‚  - Occasion Detector                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI TRACK PLANNER                                â”‚
â”‚  - LLM-powered track configuration               â”‚
â”‚  - Respects user track count                     â”‚
â”‚  - Cultural context aware                        â”‚
â”‚  - Returns List[TrackConfig]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SECTION STRUCTURE GENERATOR                     â”‚
â”‚  - Duration-aware structure                      â”‚
â”‚  - Genre-appropriate sections                    â”‚
â”‚  - Energy/density curves                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENTIC TRACK GENERATORS (NEW)                  â”‚
â”‚  - MelodyAgent: Motif-based composition          â”‚
â”‚  - HarmonyAgent: Voice-leading aware             â”‚
â”‚  - BassAgent: Chord-following                    â”‚
â”‚  - DrumAgent: Context-adaptive rhythms           â”‚
â”‚  - Each agent has creative autonomy              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HUMANIZATION ENGINE (V1 preserved)              â”‚
â”‚  - Micro-timing variations                       â”‚
â”‚  - Velocity dynamics                             â”‚
â”‚  - Groove & swing                                â”‚
â”‚  - Musical expression                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MIDI ASSEMBLY                                   â”‚
â”‚  - Assemble ALL tracks                           â”‚
â”‚  - Validate track count                          â”‚
â”‚  - Export to .mid file                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
MIDI File Output (Unique Every Time!)
```

### Core Component: Variation Engine

```python
import time
import random
import hashlib
from typing import Dict, Any

class VariationEngine:
    """
    CRITICAL COMPONENT: Ensures every generation is unique.
    Fixes V1's fatal flaw of deterministic output.
    """
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.generation_count = 0
        self.previous_seeds = []
    
    def initialize_generation(self) -> int:
        """
        Create unique seed for this generation.
        Combines: timestamp + session + generation count
        
        Guarantees uniqueness even for rapid-fire generations.
        """
        self.generation_count += 1
        
        # Create seed from multiple entropy sources
        timestamp_ns = time.time_ns()  # Nanosecond precision
        session_hash = int(hashlib.md5(self.session_id.encode()).hexdigest()[:8], 16)
        
        seed = timestamp_ns + session_hash + self.generation_count
        
        # Seed Python's random module
        random.seed(seed)
        
        # Store for reproducibility (debug mode)
        self.previous_seeds.append(seed)
        
        print(f"ðŸŽ² Generation #{self.generation_count} | Seed: {seed}")
        
        return seed
    
    def get_variation_factor(self, base_value: float, variance: float = 0.2) -> float:
        """
        Apply controlled randomization to a base value.
        
        Args:
            base_value: Starting value
            variance: Max variation as percentage (0.2 = Â±20%)
        
        Returns:
            Randomized value within variance range
        """
        variation = random.uniform(-variance, variance)
        return base_value * (1.0 + variation)
    
    def choose_weighted(self, options: list, weights: list = None):
        """Weighted random choice with better distribution."""
        if weights is None:
            weights = [1.0] * len(options)
        return random.choices(options, weights=weights, k=1)[0]
    
    def should_trigger(self, probability: float) -> bool:
        """Random boolean with specified probability."""
        return random.random() < probability
```

### Core Component: Agentic Track Generators

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class TrackAgent(ABC):
    """
    Base class for intelligent track generators.
    Each agent is autonomous and makes creative decisions.
    """
    
    def __init__(self, variation_engine: VariationEngine):
        self.variation = variation_engine
        self.memory = {}  # Agent's working memory
    
    @abstractmethod
    def generate(self, context: Dict[str, Any]) -> List[Note]:
        """
        Generate notes based on musical context.
        
        Context includes:
        - root, mode, scale
        - section (intro/verse/chorus/bridge/outro)
        - energy_level, density_level
        - bars, tempo
        - other_tracks (for coordination)
        """
        pass
    
    def get_musical_context(self, section: Section, root: int, mode: str) -> Dict:
        """Extract relevant musical information."""
        return {
            "root": root,
            "mode": mode,
            "scale": self._get_scale(mode),
            "section": section,
            "energy": section.energy_level,
            "density": section.density_level,
            "bars": section.end_bar - section.start_bar,
            "characteristics": section.characteristics
        }
    
    def _get_scale(self, mode: str) -> List[int]:
        """Get scale intervals for mode."""
        return SCALES.get(mode, SCALES["major"])


class MelodyAgent(TrackAgent):
    """
    Intelligent melody generator using motif development.
    Not just random notes - actual musical composition.
    """
    
    def generate(self, context: Dict[str, Any]) -> List[Note]:
        """Generate melodic line with motif repetition and variation."""
        
        notes = []
        scale = context["scale"]
        root = context["root"]
        section = context["section"]
        bars = context["bars"]
        energy = context["energy"]
        
        # Create initial motif (2-4 note pattern)
        motif_length = random.randint(2, 4)
        motif = self._create_motif(scale, root, motif_length)
        
        # Store in memory for this section
        self.memory["current_motif"] = motif
        
        # Generate melody by developing motif
        beat = 0.0
        total_beats = bars * 4
        
        while beat < total_beats:
            # Decide: repeat motif, vary it, or create contrast
            action = self.variation.choose_weighted(
                ["repeat", "transpose", "rhythmic_variation", "new_phrase"],
                [0.4, 0.3, 0.2, 0.1]  # Favor repetition & variation
            )
            
            if action == "repeat":
                phrase = self._repeat_motif(motif, beat)
            elif action == "transpose":
                phrase = self._transpose_motif(motif, beat, scale)
            elif action == "rhythmic_variation":
                phrase = self._vary_rhythm(motif, beat)
            else:
                phrase = self._create_new_phrase(scale, root, beat)
            
            notes.extend(phrase)
            beat += sum(note.duration for note in phrase)
        
        # Apply musical expression
        notes = self._add_phrasing(notes, section)
        
        return notes
    
    def _create_motif(self, scale: List[int], root: int, length: int) -> List[Note]:
        """Create memorable motif."""
        motif = []
        for i in range(length):
            degree = random.choice(scale)
            octave = random.choice([0, 12])
            pitch = root + degree + octave
            duration = random.choice([0.5, 1.0, 0.75])
            velocity = int(self.variation.get_variation_factor(80, 0.15))
            
            motif.append(Note(pitch, 0, duration, velocity))
        
        return motif
    
    def _repeat_motif(self, motif: List[Note], start_beat: float) -> List[Note]:
        """Exact repetition (with slight humanization)."""
        repeated = []
        for note in motif:
            new_note = Note(
                pitch=note.pitch,
                start_time=start_beat + note.start_time,
                duration=note.duration,
                velocity=int(self.variation.get_variation_factor(note.velocity, 0.1))
            )
            repeated.append(new_note)
        return repeated
    
    def _transpose_motif(self, motif: List[Note], start_beat: float, scale: List[int]) -> List[Note]:
        """Transpose motif to different scale degree."""
        transpose = random.choice([-7, -5, -3, 3, 5, 7])  # Musical intervals
        transposed = []
        for note in motif:
            new_note = Note(
                pitch=note.pitch + transpose,
                start_time=start_beat + note.start_time,
                duration=note.duration,
                velocity=note.velocity
            )
            transposed.append(new_note)
        return transposed
    
    def _add_phrasing(self, notes: List[Note], section: Section) -> List[Note]:
        """Add musical phrasing (crescendos, accents, etc.)."""
        if not notes:
            return notes
        
        # Crescendo if building energy
        if "build" in section.characteristics:
            for i, note in enumerate(notes):
                progress = i / len(notes)
                note.velocity = int(note.velocity * (0.6 + 0.4 * progress))
        
        # Accent downbeats
        for note in notes:
            if note.start_time % 4.0 < 0.1:  # Downbeat
                note.velocity = min(127, int(note.velocity * 1.3))
        
        return notes


class BassAgent(TrackAgent):
    """
    Intelligent bass generator that follows chord progressions.
    """
    
    def generate(self, context: Dict[str, Any]) -> List[Note]:
        """Generate bass line following harmony."""
        
        notes = []
        root = context["root"]
        section = context["section"]
        bars = context["bars"]
        genre = context.get("genre", "pop")
        
        # Get chord progression for genre
        progression = CHORD_PROGRESSIONS.get(genre, CHORD_PROGRESSIONS["pop"])
        
        # Generate bass pattern
        beats_per_chord = 4
        beat = 0.0
        chord_idx = 0
        
        while beat < bars * 4:
            chord = progression[chord_idx % len(progression)]
            bass_root = root + chord[0] - 24  # Two octaves down
            
            # Choose pattern based on section energy
            if section.energy_level > 0.7:
                # Active pattern
                pattern = self._walking_bass(bass_root, chord, beats_per_chord, beat)
            else:
                # Simple pattern
                pattern = self._simple_bass(bass_root, beats_per_chord, beat)
            
            notes.extend(pattern)
            beat += beats_per_chord
            chord_idx += 1
        
        return notes
    
    def _walking_bass(self, root: int, chord: List[int], duration: float, start: float) -> List[Note]:
        """Create walking bass pattern."""
        notes = []
        subdivisions = int(duration * 2)  # 8th notes
        
        for i in range(subdivisions):
            if i == 0:
                pitch = root  # Root on downbeat
            else:
                # Walk through chord tones and passing tones
                pitch = root + random.choice([0, 2, 4, 7, 9, 12])
            
            beat_pos = start + (i * 0.5)
            notes.append(Note(
                pitch=pitch,
                start_time=beat_pos,
                duration=0.45,  # Slightly staccato
                velocity=int(self.variation.get_variation_factor(85, 0.1))
            ))
        
        return notes
    
    def _simple_bass(self, root: int, duration: float, start: float) -> List[Note]:
        """Simple root note bass."""
        return [Note(
            pitch=root,
            start_time=start,
            duration=duration * 0.9,
            velocity=int(self.variation.get_variation_factor(80, 0.1))
        )]


class DrumAgent(TrackAgent):
    """
    Intelligent drum generator with context-adaptive patterns.
    """
    
    def generate(self, context: Dict[str, Any]) -> List[Note]:
        """Generate drum pattern adapted to section."""
        
        notes = []
        section = context["section"]
        bars = context["bars"]
        energy = section.energy_level
        
        # Choose pattern complexity based on energy
        if energy < 0.5:
            pattern = self._simple_pattern(bars)
        elif energy < 0.8:
            pattern = self._standard_pattern(bars)
        else:
            pattern = self._complex_pattern(bars)
        
        # Add fills at section boundaries
        if bars >= 4:
            fill = self._create_fill(bars * 4 - 2, energy)
            pattern.extend(fill)
        
        return pattern
    
    def _standard_pattern(self, bars: int) -> List[Note]:
        """Standard drum pattern."""
        notes = []
        
        for bar in range(bars):
            bar_start = bar * 4
            
            # Kick on 1 and 3
            notes.append(Note(36, bar_start + 0.0, 0.25, 100, channel=9))
            notes.append(Note(36, bar_start + 2.0, 0.25, 95, channel=9))
            
            # Snare on 2 and 4
            notes.append(Note(38, bar_start + 1.0, 0.25, 90, channel=9))
            notes.append(Note(38, bar_start + 3.0, 0.25, 92, channel=9))
            
            # Hi-hats on 8th notes
            for eighth in range(8):
                velocity = int(self.variation.get_variation_factor(70, 0.2))
                notes.append(Note(42, bar_start + (eighth * 0.5), 0.25, velocity, channel=9))
        
        return notes
```

### Integration Point: Fixed Track Generation

```python
def _generate_tracks_from_plan(
    self,
    track_plan: List[TrackConfig],
    root: int,
    mode: str,
    bars: int,
    energy: str,
    genre: str,
    sections: List[Section],
    variation_engine: VariationEngine
) -> List[Track]:
    """
    Generate ALL tracks from plan using agentic approach.
    
    CRITICAL FIX: Ensures ALL planned tracks are generated and returned.
    """
    
    # Initialize generation
    seed = variation_engine.initialize_generation()
    
    tracks = []
    
    print(f"ðŸŽµ Generating {len(track_plan)} tracks with seed {seed}")
    
    # Create agent instances
    agents = {
        "lead": MelodyAgent(variation_engine),
        "counter_melody": MelodyAgent(variation_engine),
        "harmony": HarmonyAgent(variation_engine),
        "bass": BassAgent(variation_engine),
        "drums": DrumAgent(variation_engine),
        "arpeggio": ArpeggioAgent(variation_engine),
        "pad": PadAgent(variation_engine),
    }
    
    # Generate each track
    for i, config in enumerate(track_plan):
        print(f"  Track {i+1}/{len(track_plan)}: {config.track_type} - {config.instrument}")
        
        # Get appropriate agent
        agent = agents.get(config.track_type, MelodyAgent(variation_engine))
        
        # Assign channel
        channel = 9 if config.track_type == "drums" else (i % 9)
        if channel == 9 and config.track_type != "drums":
            channel = (channel + 1) % 16
        
        # Get instrument program
        instrument_key = config.instrument.lower().replace(" ", "_")
        program = GM_INSTRUMENTS.get(instrument_key, 0)
        
        # Generate notes for ALL sections
        all_notes = []
        for section in sections:
            context = {
                "root": root,
                "mode": mode,
                "scale": SCALES.get(mode, SCALES["major"]),
                "section": section,
                "bars": section.end_bar - section.start_bar,
                "energy": section.energy_level,
                "density": section.density_level,
                "genre": genre,
            }
            
            # Generate section notes
            section_notes = agent.generate(context)
            
            # Offset to section start
            offset = section.start_bar * 4
            for note in section_notes:
                note.start_time += offset
            
            all_notes.extend(section_notes)
        
        # Apply humanization
        all_notes = self._humanize_notes(all_notes, config.track_type)
        
        # Create track
        track = Track(
            name=f"{config.instrument.title()} ({config.track_type})",
            notes=all_notes,
            midi_program=program,
            channel=channel,
            track_type=config.track_type
        )
        
        tracks.append(track)
        print(f"    âœ“ Generated {len(all_notes)} notes")
    
    print(f"âœ“ Total tracks generated: {len(tracks)}")
    
    # VALIDATION: Ensure count matches
    if len(tracks) != len(track_plan):
        raise ValueError(f"Track count mismatch! Plan: {len(track_plan)}, Generated: {len(tracks)}")
    
    return tracks
```

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Identical prompts produce >30% different note patterns across 10 generations (measured by edit distance)
- **SC-002**: Requested track count matches generated track count 100% of the time (no failures)
- **SC-003**: Cultural music requests use correct instruments/scales â‰¥90% accuracy (tested across 20+ cultures)
- **SC-004**: Generated music has measurable section variation (energy/density differs by â‰¥20% between intro/chorus)
- **SC-005**: All generated notes have humanization (timing variance 5-20ms, velocity variance 5-15)
- **SC-006**: Duration accuracy within Â±5% of requested (120s request â†’ 114-126s output)
- **SC-007**: User satisfaction rating â‰¥4.5/5 for "music sounds natural and varied"
- **SC-008**: Zero crashes or errors during track generation (100% reliability)
- **SC-009**: Generation completes in <10 seconds for standard 2-minute, 5-track composition
- **SC-010**: Session-based modifications work correctly (add/modify/extend) â‰¥95% of the time

## Implementation Priority

### Phase 1: Critical Fixes (Week 1)
1. âœ… Implement VariationEngine with proper seeding
2. âœ… Fix track count bug (ensure ALL tracks generated)
3. âœ… Add validation and error handling
4. âœ… Test: Same prompt â†’ Different outputs

### Phase 2: Agentic Generation (Week 2)
5. âœ… Implement TrackAgent base class
6. âœ… Implement MelodyAgent with motif development
7. âœ… Implement BassAgent with chord following
8. âœ… Implement DrumAgent with adaptive patterns
9. âœ… Test: Agents produce musically coherent output

### Phase 3: Preserve V1 Features (Week 3)
10. âœ… Integrate duration parser (V1 preserved)
11. âœ… Integrate cultural knowledge base (V1 preserved)
12. âœ… Integrate humanization (V1 preserved)
13. âœ… Integrate section structure (V1 preserved)
14. âœ… Test: All V1 features work in V2

### Phase 4: Session Management (Week 4)
15. âœ… Implement session state management
16. âœ… Implement add/modify/extend actions
17. âœ… Test: Conversational workflow

## Testing Strategy

### Unit Tests
- `test_variation_engine_uniqueness`: Verify seeds are unique across 1000 generations
- `test_track_count_accuracy`: Verify requested count matches output for 1-8 tracks
- `test_cultural_detection`: Verify cultural styles recognized correctly
- `test_duration_parsing`: Verify all duration formats parsed correctly
- `test_humanization`: Verify timing/velocity variations applied

### Integration Tests
- `test_full_generation_uniqueness`: Same prompt 10 times â†’ 10 different outputs
- `test_multi_track_complete`: Request 7 tracks â†’ Get all 7 in MIDI
- `test_cultural_authenticity`: "Japanese music" â†’ Correct instruments/scales
- `test_section_variation`: Analyze output â†’ Sections differ measurably
- `test_agent_coordination`: Multi-track â†’ Musically coherent

### User Acceptance Tests
- Musicians evaluate: "Does this sound human-composed?" (â‰¥80% yes)
- Non-musicians test: "Can you create party music?" (â‰¥90% success)
- Cultural experts verify: "Is this authentic?" (â‰¥75% yes for their culture)
- Stress test: 100 generations â†’ All unique

## Non-Goals

- Real-time streaming generation (acceptable latency: <10s)
- Audio rendering (MIDI only)
- Music notation display
- User accounts or cloud storage
- Mixing/mastering (MIDI velocity only)

## Migration from V1

### Breaking Changes
- None - V2 is backward compatible with V1 prompts
- All V1 features preserved and enhanced

### Data Migration
- No data migration needed (stateless, file-based output)

### Deprecation
- V1 code archived in `app.py.backup`
- V2 becomes primary in `app.py`

---

**End of Specification**

This specification provides a complete blueprint for MidiGent V2 that fixes the critical randomization bug, implements true agentic generation, and preserves/enhances all V1 features.
